{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LX-UTagger to annotate a text from the BDCamões corpus\n",
    "This is an example notebook that illustrates how you can use the LX-UTagger web service to annotate\n",
    "a sample text from the BDCamões corpus (the full corpus is [available from the PORTULAN CLARIN repository](https://portulanclarin.net/repository/browse/bdcamoes-corpus-collection-of-portuguese-literary-documents-from-the-digital-library-of-camoes-ip-part-i/52f2b16412c411ea8a1302420a000005407eb504ccc045a4a0582ab53dfd43fd/)).\n",
    "\n",
    "**Before you run this example**, replace `access_key_goes_here` by your webservice access key, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LXUTAGGER_WS_API_KEY = 'access_key_goes_here'\n",
    "LXUTAGGER_WS_API_URL = 'https://portulanclarin.net/workbench/lx-utagger/api/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required Python modules\n",
    "The next cell will take care of installing the `requests` and `matplotlib` packages,\n",
    "if not already installed, and make them available to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests\n",
    "except:\n",
    "    !pip3 install requests\n",
    "    import requests\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    !pip3 install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping the complexities of the JSON-RPC API in a simple, easy to use function\n",
    "\n",
    "The `WSException` class defined below, will be used later to identify errors\n",
    "from the webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSException(Exception):\n",
    "    'Webservice Exception'\n",
    "    def __init__(self, errordata):\n",
    "        \"errordata is a dict returned by the webservice with details about the error\"\n",
    "        super().__init__(self)\n",
    "        assert isinstance(errordata, dict)\n",
    "        self.message = errordata[\"message\"]\n",
    "        # see https://json-rpc.readthedocs.io/en/latest/exceptions.html for more info\n",
    "        # about JSON-RPC error codes\n",
    "        if -32099 <= errordata[\"code\"] <= -32000:  # Server Error\n",
    "            if errordata[\"data\"][\"type\"] == \"WebServiceException\":\n",
    "                self.message += f\": {errordata['data']['message']}\"\n",
    "            else:\n",
    "                self.message += f\": {errordata['data']!r}\"\n",
    "    def __str__(self):\n",
    "        return self.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function invokes the LX-Tagger webservice through its public JSON-RPC API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(text, format):\n",
    "    '''\n",
    "    Arguments\n",
    "        text: a string with a maximum of 4000 characters, Portuguese text, with\n",
    "             the input to be processed\n",
    "        format: either 'CONLL', 'column' or 'JSON'\n",
    "\n",
    "    Returns a string with the output according to specification in\n",
    "       https://portulanclarin.net/workbench/lx-utagger/\n",
    "    \n",
    "    Raises a WSException if an error occurs.\n",
    "    '''\n",
    "\n",
    "    request_data = {\n",
    "        'method': 'tag',\n",
    "        'jsonrpc': '2.0',\n",
    "        'id': 0,\n",
    "        'params': {\n",
    "            'text': text,\n",
    "            'format': format,\n",
    "            'key': LXUTAGGER_WS_API_KEY,\n",
    "        },\n",
    "    }\n",
    "    request = requests.post(LXUTAGGER_WS_API_URL, json=request_data)\n",
    "    response_data = request.json()\n",
    "    if \"error\" in response_data:\n",
    "        raise WSException(response_data[\"error\"])\n",
    "    else:\n",
    "        return response_data[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the function we just defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Esta frase serve para testar o funcionamento do tagger. Esta outra\n",
    "frase faz o mesmo.'''\n",
    "# the CONLL annotation format is a popular format for annotating part of speech\n",
    "result = tag(text, format=\"CONLL\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The JSON output format\n",
    "\n",
    "The JSON format (which we obtain by passing `format=\"JSON\"` into the `annotate` function) is more\n",
    "convenient when we need to further process the annotations, because each abstraction is mapped\n",
    "directly into a Python native object (lists, dicts, strings, etc) as follows:\n",
    "- The returned object is a `list`, where each element corresponds to a paragraph of the given text;\n",
    "- In turn, each paragraph is a `list` where each element represents a sentence;\n",
    "- Each sentence is a `list` where each element represents a token;\n",
    "- Each token is a `dict` where each key-value pair is an attribute of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_text = tag(text, format=\"JSON\")\n",
    "for pnum, paragraph in enumerate(annotated_text, start=1): # enumerate paragraphs in text, starting at 1\n",
    "    print(f\"paragraph {pnum}:\")\n",
    "    for snum, sentence in enumerate(paragraph, start=1): # enumerate sentences in paragraph, starting at 1\n",
    "        print(f\"  sentence {snum}:\")\n",
    "        for tnum, token in enumerate(sentence, start=1): # enumerate tokens in sentence, starting at 1\n",
    "            print(f\"    token {tnum}: {token!r}\")  # print a token representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preparing our working text\n",
    "\n",
    "In the next code cell, we will download a copy of the book \"Viagens na minha terra\" and prepare it to be used as our working text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plain text version of this book is available from our Gitbub repository:\n",
    "sample_text_url = \"https://github.com/portulanclarin/jupyter-notebooks/raw/main/sample-data/viagensnaminhaterra.txt\"\n",
    "\n",
    "req = requests.get(sample_text_url)\n",
    "sample_text_lines = req.text.splitlines()\n",
    "\n",
    "num_lines = len(sample_text_lines)\n",
    "print(f\"The downloaded text contains {num_lines} lines\")\n",
    "\n",
    "# discard whitespace at beginning and end of each line:\n",
    "sample_text_lines = [line.strip() for line in sample_text_lines]\n",
    "\n",
    "# discard empty lines\n",
    "sample_text_lines = [line for line in sample_text_lines if line]\n",
    "\n",
    "# how many lines do we have left?\n",
    "num_lines = len(sample_text_lines)\n",
    "print(f\"After discarding empty lines we are left with {num_lines} non-empty lines\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating with the LX-UTagger web service\n",
    "\n",
    "There is a limit on the number of web service requests per hour that can be made in association with any given key.\n",
    "Thus, we should send as much text as possible in each request while also conforming with the 4000 characters\n",
    "per request limit.\n",
    "\n",
    "To this end, the following function slices our text into chunks smaller than 4K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_into_chunks(lines, max_chunk_size=4000):\n",
    "    chunk, chunk_size = [], 0\n",
    "    for lnum, line in enumerate(lines, start=1):\n",
    "        if (chunk_size + len(line)) <= max_chunk_size:\n",
    "            chunk.append(line)\n",
    "            chunk_size += len(line) + 1\n",
    "            # the + 1 above is for the newline character terminating each line\n",
    "        else:\n",
    "            yield \"\\n\".join(chunk)\n",
    "            if len(line) > max_chunk_size:\n",
    "                print(f\"line {lnum} is longer than 4000 characters; truncating\")\n",
    "                line = line[:4000]\n",
    "            chunk, chunk_size = [line], len(line) + 1\n",
    "    if chunk:\n",
    "        yield \"\\n\".join(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will apply `slice_into_chunks` to the sample text to get the chunks to be annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(slice_into_chunks(sample_text_lines))\n",
    "annotated_text = [] # annotated paragraphs will be stored here\n",
    "chunks_processed = 0  # this variable keeps track of which chunks have been processed already\n",
    "print(f\"There are {len(chunks)} chunks to be annotated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will invoke `annotate` on each chunk.\n",
    "If we get an exception while annotating a chunk:\n",
    "- check the exception message to determine what was the cause;\n",
    "- if the maximum number of requests per hour has been exceeded, then wait some time before retrying;\n",
    "- if a temporary error occurred in the webservice, try again later.\n",
    "\n",
    "In any case, as long as the notebook is not shutdown or restarted, the text that has been annotated thus far is not lost,\n",
    "and re-running the following cell will pick up from the point where the exception occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnum, chunk in enumerate(chunks[chunks_processed:], start=chunks_processed+1):\n",
    "    try:\n",
    "        annotated_text.extend(tag(chunk, format=\"JSON\"))\n",
    "        chunks_processed = cnum\n",
    "        # print one dot for each annotated chunk to get some progress feedback\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "    except Exception as exc:\n",
    "        chunk_preview = chunk[:100] + \"[...]\" if len(chunk) > 100 else chunk\n",
    "        print(\n",
    "            f\"\\nError: annotation of chunk {cnum} failed ({exc}); chunk contents:\\n\\n{chunk_preview}\\n\\n\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a pie chart with the most common part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "tag_frequencies = collections.Counter(\n",
    "        token[\"upos\"]\n",
    "        for paragraph in annotated_text\n",
    "        for sentence in paragraph\n",
    "        for token in sentence\n",
    ").most_common()\n",
    "\n",
    "tags = [tag for tag, _ in tag_frequencies[:9]]\n",
    "freqs = [freq for _, freq in tag_frequencies[:9]]\n",
    "\n",
    "tags.append(\"other\")\n",
    "freqs.append(sum(freq for _, freq in tag_frequencies[10:]))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(freqs, labels=tags, autopct='%1.1f%%', startangle=90)\n",
    "ax1.axis('equal')  # equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()\n",
    "# To learn more about matplotlib visit https://matplotlib.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the status of a webservice access key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_status():\n",
    "    '''Returns a string with the detailed status of the webservice access key'''\n",
    "    \n",
    "    request_data = {\n",
    "        'method': 'key_status',\n",
    "        'jsonrpc': '2.0',\n",
    "        'id': 0,\n",
    "        'params': {\n",
    "            'key': LXUTAGGER_WS_API_KEY,\n",
    "        },\n",
    "    }\n",
    "    request = requests.post(LXUTAGGER_WS_API_URL, json=request_data)\n",
    "    response_data = request.json()\n",
    "    if \"error\" in response_data:\n",
    "        raise WSException(response_data[\"error\"])\n",
    "    else:\n",
    "        return response_data[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_key_status()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "006d5deb8e6cdcd4312641bdf15f3bc20f0769a7305d81173599a7b40f33b4a2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
